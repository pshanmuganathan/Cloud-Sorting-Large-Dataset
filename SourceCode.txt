1. Shared Memory

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import java.io.File;
import java.io.FileReader;
import java.io.FileWriter;

public class SharedMem 
{
	public static int no_threads;
	private static List <String> chunk_list = new ArrayList<String>();
	public static int chunk_count = 80;
	public static long fileLinescount;
	public static String filepath= "temp";
	public static List <String> sortedlist = new ArrayList <String> ();

	public static void main(String[] args) throws Exception 
	{

no_threads = Integer.parseInt(args[0]);	
List <String> dataToSort = new ArrayList <String> ();



//Get the InputFiles ize
File file = new File("InputFile.txt");
fileLinescount = file.length();
file.delete();
//Creat the chunk list by giving them the same names as they used to generate through the GNU split command
	for(int i = 0; i<chunk_count; i++)
	{
			chunk_list.add("chunk-"+String.format("%02d", i)+".txt");	
	}
double startTime=System.currentTimeMillis();
//Creation of threads based on the user threadcount	
Thread [] thread = new Thread[no_threads];
		
		for (int i= 0 ;i <no_threads; i++)
		{
			
			thread[i] = new Thread(new Runnable()
			{	
				@Override
				public void run()
				{
					try
					{
						for(int i=0;i<chunk_count/no_threads;i++)
						{
							//Read the chunk File
							int x = ((Integer.parseInt(Thread.currentThread().getName())*chunk_count)/(no_threads))+i;
							  filepath = chunk_list.get(x);
							
							FileReader file = new FileReader(new File(filepath));
							BufferedReader bufRead = new BufferedReader(file);
							String line = null;
		
						while ((line = bufRead.readLine()) != null)
						{
							dataToSort.add(line);
			
						}
								//After Reading Quicksort the contents of each chunk file
							sortedlist=QuickSort(dataToSort,0,dataToSort.size()-1);
							//Once Quicksort is performed write the proper sorted output to the file again
							FileWriter filewrite = new FileWriter(new File(chunk_list.get(x)));
							BufferedWriter bufWrite = new BufferedWriter(filewrite);
							for(String str : sortedlist)
							{
								bufWrite.write(str+" "+"\n");
							}
							bufWrite.close();
							sortedlist.clear();
						}
					}
					catch(Exception e)	
					{
					}
				}
			});
			
			thread[i].setName(Integer.toString(i));
			thread[i].start();
		}
	
		//Wait for other threads to complete and join
		for(int j =0;j<no_threads;j++)
		{
			thread[j].join();
		}
		MergeSort();
			double endtime=System.currentTimeMillis();
		System.out.println("Time Taken in millis: "+(endtime-startTime));
	}

static List<String> QuickSort(List<String> dataToSort, int low, int high) throws IOException
	{
		int i = low;
		int j = high;
	//Center element as pivot
		String pivot = dataToSort.get(low + (high-low)/2).substring(0, 10);

		
		while(i <= j)
		{
			String temp = null;
			
			while(dataToSort.get(i).substring(0, 10).compareTo(pivot) < 0)
			{
				i++;
			}

			
			while(dataToSort.get(j).substring(0, 10).compareTo(pivot) > 0)
			{
				j--;
			}
		
			
			if(i<=j)
			{
				//Swap
				temp = dataToSort.get(i);
				dataToSort.set(i, dataToSort.get(j));
				dataToSort.set(j,temp);

				
				i++;
				j--;
			}
		}

		//Recursive Quicksort functions
		if (low < j)
			QuickSort(dataToSort,low, j);

	
		if (i < high)
			QuickSort(dataToSort,i, high);

		return dataToSort;
	}
private static void MergeSort() throws IOException 
	{
BufferedReader[] bufReaderArray = new BufferedReader[chunk_count];
		List <String> mergeList = new ArrayList<String>();
		List <String> keyValueList = new ArrayList<String>();
		
		
		//Copy first line from all the chunks and find minimum value among these chunks
		for (int i=0; i<chunk_count; i++)
		{
			bufReaderArray[i] = new BufferedReader(new FileReader(chunk_list.get(i)));

			String fileLine = bufReaderArray[i].readLine();

			if(fileLine != null)
			{
				mergeList.add(fileLine.substring(0, 10));
				keyValueList.add(fileLine);
			}
		}

		//Final Output File which will be sorted
		BufferedWriter bufw = new BufferedWriter(new FileWriter("MergedOutput.txt"));

		
//Merge and Sort Algorithm
		for(long j=0; j<fileLinescount;j++)
		{
			String minString = mergeList.get(0);
			int minFile = 0;

			for (int k = 0; k< chunk_count; k++)
			{
				if (minString.compareTo(mergeList.get(k)) > 0)
				{
					minString = mergeList.get(k);
					minFile = k;
				}
			}

			bufw.write(keyValueList.get(minFile)+"\n");
			mergeList.set(minFile,"-1");
			keyValueList.set(minFile,"-1");

			String temp = bufReaderArray[minFile].readLine();

			if (temp != null)
			{
				mergeList.set(minFile,temp.substring(0, 10));
				keyValueList.set(minFile, temp);
			}
			//if one of the files get finished earlier than others, copy other values and Quick Sort over them
				//write the output to the final sorted file
			else
			{
				
				j = fileLinescount;

				List <String> tempList = new ArrayList<String>();

				for(int i = 0;i< mergeList.size();i++)
				{
					if(keyValueList.get(i)!="-1")
						tempList.add(keyValueList.get(i));

					while((minString = bufReaderArray[i].readLine())!=null)
					{
						tempList.add(minString);
					}
				}

				tempList = QuickSort(tempList, 0, tempList.size()-1);
				int i =0;
				while(i < tempList.size())
				{
					bufw.write(tempList.get(i)+"\n");
					i++;
				}
				
				
			}
		}
		bufw.close();
	
		for(int i=0; i<chunk_count;i++)
			bufReaderArray[i].close();
	}

}







2. Haddop Sort

/*
 * HADOOP MAP-REDUCE
 * Mapper will generate the (key,value) pair of the data read from the text file
 * Reducer will generate the sorted Data obtained from Sort & Shuffle Phase
 */
import java.io.IOException;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.FileOutputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.mapred.lib.IdentityReducer;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;

public class TeraSort extends Configured implements Tool
{
        public int run(String[] args) throws Exception
        {
                // To omit the line which has length less than 2
                // i.e if Args are correctly specified or Not
                if(args.length!=2)
                {        // To Run the Hadoop Code
					 System.out.println("USAGE: hadoop jar <JAR_FILE> <HDFS INPUT FILE PATH> <OUTPUT DIRECTORY PATH>");
                        return -1;
                }
                // Job Configuration
                JobConf jobConf = new JobConf(TeraSortHadoop.class);
                // Set the JobName as class name
                jobConf.setJobName(this.getClass().getName());
                // Set the File Paths Input and Output
                FileInputFormat.setInputPaths(jobConf, new Path(args[0]));
                FileOutputFormat.setOutputPath(jobConf, new Path(args[1]));
                // Configure the Mapper data class
                jobConf.setMapperClass(MapperSort.class);
                jobConf.setMapOutputKeyClass(Text.class);
                jobConf.setOutputKeyClass(Text.class);
                // Configure the Reduce data Class
                jobConf.setReducerClass(IdentityReducer.class);
                jobConf.setMapOutputValueClass(Text.class);
                jobConf.setOutputValueClass(NullWritable.class);
                // Run the Job on HADOOP
                JobClient.runJob(jobConf);
                return 0;
        }
		
		
        public static void main(String[] args) throws Exception
        {
                int exitCode = ToolRunner.run(new TeraSortHadoop(), args);
                System.exit(exitCode);
        }

        // Mapper Class extending MapReduce Class implementation
        public static class MapperSort extends MapReduceBase implements Mapper<LongWritable, Text, Text, Text>
        {
                
				private Text finalKey = new Text();
                private Text finalValue = new Text();
				// Constructor
                public MapperSort()
                {
                }

                // It will map Key 10 bytes, Value 90 Bytes output to reducer
                public void map(LongWritable Key, Text Value, OutputCollector<Text, Text> output, Reporter rep) throws IOException
                {
                        // Convert to String and split the data
                        String line = Value.toString();
                        // Fetch from data Key and Value
                        String newKey = line.substring(0, 10);
                        String newKey = line.substring(10,98);
                        //Give as the output to next phase
                        word.finalKey(newKey);
						word1.finalValue(newKey);
						context.write(finalKey, finalValue);
                }
        }

        // Reducer class
        public static class ReducerSort extends MapReduceBase implements Reducer<LongWritable, Text, Text, Text>
        {
                private Text finalKey = new Text();
                private Text finalValue = new Text();
				// Constructor
                public ReducerSort()
                {
                }
                public void reduce(Text key, Text value, Context context) throws IOException, InterruptedException
                {
                        finalKey.set(key.toString() + value.toString());
                        finalValue.set("");
                        context.write(finalKey, finalValue);
                }
        }
}

                       




3. Spark Sort

//Spark also uses the same Map-Reduce based implementation as Hadoop does.
//When installing Spark it installs various features and language packages, like Scala, R-Studio, Python, specifically for Spark.
//Implemented sorting using scala’s sortByKey(), functionality, which sorts the data based on Key value (10 Bytes). The sorted data obtained using spark, is divided into chunks.

import scala.collection.immutable.ListMap
import java.io.{File, BufferedWriter, FileWriter}
import org.apache.spark.SparkContext._
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf

object SparkSort {

  def main(args: Array[String]) 
  {
	//Should be some file on your system
    val logFile = "spark-1.6.1-bin-hadoop2.6/input" 
	
	//configur the spark
    val conf = new SparkConf().setAppName("File Sort").set("spark.executor.memory", "2g")
    val sc = new SparkContext(conf)
    val file = new File("ouput")
    val bw = new BufferedWriter(new FileWriter(file))
    val data = sc.textFile(logFile)
	
	//It will split the file
    val splitData = data.flatMap(line => line.split("\n"))

    val arrayFromCollectedData = splitData.flatMap(line => Map(line.substring(0,10) -> line.substring(10,line.length)))
    val mapFromArray = arrayFromCollectedData.map(line => line._1 -> line._2)
    val sortedData = mapFromArray.sortBy(_._1)
    val collectedData = sortedData.collect()
    collectedData.foreach(line => bw.write(line._1 + line._2 + "\r\n"))

  }
}